{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Clustering Techniques with SUBMARIT\n",
    "\n",
    "This notebook explores advanced clustering techniques and features available in SUBMARIT, including:\n",
    "- Constrained clustering\n",
    "- Multiple algorithm variants\n",
    "- Entropy-based clustering\n",
    "- GAP statistic for optimal cluster selection\n",
    "- Top-k solution analysis\n",
    "- Advanced parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SUBMARIT modules\n",
    "from submarit.algorithms import (\n",
    "    KSMLocalSearch, KSMLocalSearch2,\n",
    "    KSMLocalSearchConstrained, KSMLocalSearchConstrained2\n",
    ")\n",
    "from submarit.evaluation import (\n",
    "    GAPStatistic, EntropyClusterer, \n",
    "    ClusterEvaluator, EvaluationVisualizer\n",
    ")\n",
    "from submarit.validation import (\n",
    "    run_clusters_topk, analyze_solution_stability,\n",
    "    create_switching_matrix_distribution, k_sm_empirical_p\n",
    ")\n",
    "\n",
    "# Set style and random seed\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constrained Clustering\n",
    "\n",
    "Sometimes you need to enforce constraints on cluster sizes or membership. SUBMARIT provides constrained variants of the local search algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a more complex substitution matrix\n",
    "n_products = 20\n",
    "n_true_clusters = 4\n",
    "\n",
    "# Create block-diagonal structure with noise\n",
    "substitution_matrix = np.zeros((n_products, n_products))\n",
    "cluster_sizes = [5, 5, 6, 4]\n",
    "start_idx = 0\n",
    "\n",
    "for size in cluster_sizes:\n",
    "    end_idx = start_idx + size\n",
    "    # High within-cluster substitution\n",
    "    substitution_matrix[start_idx:end_idx, start_idx:end_idx] = np.random.uniform(0.6, 0.9, (size, size))\n",
    "    start_idx = end_idx\n",
    "\n",
    "# Add noise and between-cluster substitution\n",
    "noise = np.random.uniform(0, 0.3, (n_products, n_products))\n",
    "substitution_matrix += noise\n",
    "substitution_matrix = (substitution_matrix + substitution_matrix.T) / 2  # Symmetrize\n",
    "np.fill_diagonal(substitution_matrix, 0)\n",
    "substitution_matrix = np.clip(substitution_matrix, 0, 1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(substitution_matrix, cmap='YlOrRd', cbar_kws={'label': 'Substitution Score'})\n",
    "plt.title('Complex Substitution Matrix with 4 Submarkets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Minimum cluster size constraint\n",
    "print(\"=== Constrained Clustering with Minimum Cluster Size ===\")\n",
    "\n",
    "# Define constraints\n",
    "min_cluster_size = 3  # Each cluster must have at least 3 products\n",
    "\n",
    "# Initialize constrained local search\n",
    "constrained_search = KSMLocalSearchConstrained(\n",
    "    n_clusters=4,\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    max_iterations=200,\n",
    "    n_restarts=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run clustering\n",
    "constrained_result = constrained_search.fit(substitution_matrix)\n",
    "\n",
    "# Check cluster sizes\n",
    "cluster_sizes = [np.sum(constrained_result.labels == i) for i in range(4)]\n",
    "print(f\"\\nCluster sizes: {cluster_sizes}\")\n",
    "print(f\"All clusters meet minimum size constraint: {all(size >= min_cluster_size for size in cluster_sizes)}\")\n",
    "\n",
    "# Visualize constrained clustering\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Cluster sizes bar chart\n",
    "ax1.bar(range(4), cluster_sizes, color=['red', 'blue', 'green', 'orange'])\n",
    "ax1.axhline(y=min_cluster_size, color='black', linestyle='--', label=f'Min size = {min_cluster_size}')\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.set_ylabel('Number of Products')\n",
    "ax1.set_title('Cluster Sizes with Constraint')\n",
    "ax1.legend()\n",
    "\n",
    "# Reordered matrix\n",
    "sorted_indices = np.argsort(constrained_result.labels)\n",
    "reordered_matrix = substitution_matrix[sorted_indices][:, sorted_indices]\n",
    "sns.heatmap(reordered_matrix, ax=ax2, cmap='YlOrRd')\n",
    "ax2.set_title('Reordered Matrix (Constrained Clustering)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Must-link and cannot-link constraints\n",
    "print(\"=== Constrained Clustering with Must-Link/Cannot-Link ===\")\n",
    "\n",
    "# Define pairwise constraints\n",
    "must_link = [(0, 1), (0, 2), (10, 11)]  # These products must be in the same cluster\n",
    "cannot_link = [(0, 15), (5, 10)]  # These products cannot be in the same cluster\n",
    "\n",
    "# Initialize constrained search with pairwise constraints\n",
    "constrained_search2 = KSMLocalSearchConstrained2(\n",
    "    n_clusters=4,\n",
    "    must_link=must_link,\n",
    "    cannot_link=cannot_link,\n",
    "    max_iterations=200,\n",
    "    n_restarts=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run clustering\n",
    "constrained_result2 = constrained_search2.fit(substitution_matrix)\n",
    "\n",
    "# Verify constraints are satisfied\n",
    "print(\"\\nConstraint Verification:\")\n",
    "print(\"Must-link constraints:\")\n",
    "for i, j in must_link:\n",
    "    same_cluster = constrained_result2.labels[i] == constrained_result2.labels[j]\n",
    "    print(f\"  Products {i} and {j}: {'✓ Same cluster' if same_cluster else '✗ Different clusters'}\")\n",
    "\n",
    "print(\"\\nCannot-link constraints:\")\n",
    "for i, j in cannot_link:\n",
    "    different_cluster = constrained_result2.labels[i] != constrained_result2.labels[j]\n",
    "    print(f\"  Products {i} and {j}: {'✓ Different clusters' if different_cluster else '✗ Same cluster'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entropy-Based Clustering\n",
    "\n",
    "Entropy-based clustering maximizes the information content of the clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize entropy-based clusterer\n",
    "entropy_clusterer = EntropyClusterer(\n",
    "    n_clusters=4,\n",
    "    entropy_weight=0.5,  # Balance between cohesion and entropy\n",
    "    max_iterations=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run entropy-based clustering\n",
    "print(\"Running entropy-based clustering...\")\n",
    "entropy_result = entropy_clusterer.fit(substitution_matrix)\n",
    "\n",
    "# Compare with standard clustering\n",
    "standard_search = KSMLocalSearch(n_clusters=4, random_state=42)\n",
    "standard_result = standard_search.fit(substitution_matrix)\n",
    "\n",
    "# Calculate entropy for both solutions\n",
    "def calculate_cluster_entropy(labels):\n",
    "    \"\"\"Calculate entropy of cluster distribution\"\"\"\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / len(labels)\n",
    "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "\n",
    "entropy_standard = calculate_cluster_entropy(standard_result.labels)\n",
    "entropy_entropy_based = calculate_cluster_entropy(entropy_result.labels)\n",
    "\n",
    "print(f\"\\nEntropy comparison:\")\n",
    "print(f\"Standard clustering entropy: {entropy_standard:.4f}\")\n",
    "print(f\"Entropy-based clustering entropy: {entropy_entropy_based:.4f}\")\n",
    "print(f\"\\nCluster balance (entropy-based):\")\n",
    "for i in range(4):\n",
    "    size = np.sum(entropy_result.labels == i)\n",
    "    print(f\"  Cluster {i}: {size} products ({size/n_products*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GAP Statistic for Optimal Number of Clusters\n",
    "\n",
    "The GAP statistic helps determine the optimal number of clusters by comparing the clustering quality to a null reference distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GAP statistic calculator\n",
    "gap_calculator = GAPStatistic(\n",
    "    max_clusters=8,\n",
    "    n_references=20,  # Number of reference datasets\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calculate GAP statistic\n",
    "print(\"Calculating GAP statistic (this may take a moment)...\")\n",
    "gap_results = gap_calculator.compute(substitution_matrix)\n",
    "\n",
    "# Plot GAP statistic\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# GAP values\n",
    "k_values = gap_results.k_values\n",
    "gap_values = gap_results.gap_values\n",
    "gap_std = gap_results.gap_std\n",
    "\n",
    "ax1.errorbar(k_values, gap_values, yerr=gap_std, marker='o', capsize=5)\n",
    "ax1.axvline(x=gap_results.optimal_k, color='red', linestyle='--', \n",
    "            label=f'Optimal k = {gap_results.optimal_k}')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('GAP Statistic')\n",
    "ax1.set_title('GAP Statistic Analysis')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Within-cluster sum of squares\n",
    "ax2.plot(k_values, gap_results.wss_values, marker='o')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Within-Cluster Sum of Squares')\n",
    "ax2.set_title('Elbow Method')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal number of clusters: {gap_results.optimal_k}\")\n",
    "print(f\"GAP value at optimal k: {gap_results.gap_values[gap_results.optimal_k-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-K Solution Analysis\n",
    "\n",
    "Instead of just finding the best solution, we can analyze the top-k solutions to understand solution stability and alternative clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run top-k analysis\n",
    "print(\"Finding top-10 clustering solutions...\")\n",
    "\n",
    "topk_results = run_clusters_topk(\n",
    "    substitution_matrix,\n",
    "    n_clusters=4,\n",
    "    k=10,  # Find top 10 solutions\n",
    "    n_runs=50,  # Total number of runs\n",
    "    algorithm='KSMLocalSearch',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display top solutions\n",
    "print(\"\\nTop 10 Solutions:\")\n",
    "print(\"Rank | Objective Value | Frequency | Cumulative Freq\")\n",
    "print(\"-\" * 50)\n",
    "for i, solution in enumerate(topk_results.top_solutions):\n",
    "    print(f\"{i+1:4d} | {solution.objective:14.6f} | {solution.frequency:9d} | {solution.cumulative_frequency:15d}\")\n",
    "\n",
    "# Analyze solution stability\n",
    "stability_analysis = analyze_solution_stability(topk_results)\n",
    "\n",
    "print(f\"\\nSolution Stability Analysis:\")\n",
    "print(f\"  Diversity score: {stability_analysis.diversity_score:.4f}\")\n",
    "print(f\"  Top solution dominance: {stability_analysis.top_solution_dominance:.2%}\")\n",
    "print(f\"  Average pairwise similarity: {stability_analysis.avg_pairwise_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize solution diversity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Objective value distribution\n",
    "ax = axes[0, 0]\n",
    "objectives = [sol.objective for sol in topk_results.top_solutions]\n",
    "ax.bar(range(1, len(objectives)+1), objectives, color='steelblue')\n",
    "ax.set_xlabel('Solution Rank')\n",
    "ax.set_ylabel('Objective Value')\n",
    "ax.set_title('Top-K Solution Objectives')\n",
    "\n",
    "# 2. Frequency distribution\n",
    "ax = axes[0, 1]\n",
    "frequencies = [sol.frequency for sol in topk_results.top_solutions]\n",
    "ax.bar(range(1, len(frequencies)+1), frequencies, color='darkgreen')\n",
    "ax.set_xlabel('Solution Rank')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Solution Frequency Distribution')\n",
    "\n",
    "# 3. Pairwise similarity heatmap\n",
    "ax = axes[1, 0]\n",
    "n_solutions = min(5, len(topk_results.top_solutions))  # Show top 5\n",
    "similarity_matrix = np.zeros((n_solutions, n_solutions))\n",
    "\n",
    "for i in range(n_solutions):\n",
    "    for j in range(n_solutions):\n",
    "        if i != j:\n",
    "            labels_i = topk_results.top_solutions[i].labels\n",
    "            labels_j = topk_results.top_solutions[j].labels\n",
    "            # Calculate Rand index as similarity\n",
    "            from submarit.validation import RandIndex\n",
    "            rand_calc = RandIndex()\n",
    "            similarity_matrix[i, j] = rand_calc.compute(labels_i, labels_j).rand_index\n",
    "        else:\n",
    "            similarity_matrix[i, j] = 1.0\n",
    "\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax,\n",
    "            xticklabels=[f'Sol {i+1}' for i in range(n_solutions)],\n",
    "            yticklabels=[f'Sol {i+1}' for i in range(n_solutions)])\n",
    "ax.set_title('Pairwise Solution Similarity (Rand Index)')\n",
    "\n",
    "# 4. Cluster assignment variability\n",
    "ax = axes[1, 1]\n",
    "# Calculate how often each product changes clusters across top solutions\n",
    "assignment_variability = np.zeros(n_products)\n",
    "for i in range(n_products):\n",
    "    assignments = [sol.labels[i] for sol in topk_results.top_solutions[:5]]\n",
    "    assignment_variability[i] = len(set(assignments)) / len(assignments)\n",
    "\n",
    "ax.bar(range(n_products), assignment_variability, color='coral')\n",
    "ax.set_xlabel('Product')\n",
    "ax.set_ylabel('Assignment Variability')\n",
    "ax.set_title('Product Clustering Stability (Top 5 Solutions)')\n",
    "ax.set_xticks(range(0, n_products, 2))\n",
    "ax.set_xticklabels([f'P{i}' for i in range(0, n_products, 2)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithm Comparison\n",
    "\n",
    "Let's compare the performance of different algorithm variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms to compare\n",
    "algorithms = {\n",
    "    'KSMLocalSearch': KSMLocalSearch(n_clusters=4, random_state=42),\n",
    "    'KSMLocalSearch2': KSMLocalSearch2(n_clusters=4, random_state=42),\n",
    "    'KSMLocalSearchConstrained': KSMLocalSearchConstrained(n_clusters=4, min_cluster_size=3, random_state=42),\n",
    "    'EntropyClusterer': EntropyClusterer(n_clusters=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = {}\n",
    "evaluator = ClusterEvaluator()\n",
    "\n",
    "print(\"Comparing algorithms...\")\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nRunning {name}...\")\n",
    "    \n",
    "    # Time the execution\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    result = algorithm.fit(substitution_matrix)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = evaluator.evaluate(substitution_matrix, result.labels)\n",
    "    \n",
    "    comparison_results[name] = {\n",
    "        'result': result,\n",
    "        'eval': eval_result,\n",
    "        'time': execution_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Objective: {result.best_objective:.6f}\")\n",
    "    print(f\"  Silhouette: {eval_result.silhouette_score:.4f}\")\n",
    "    print(f\"  Time: {execution_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "metrics = ['Objective', 'Silhouette', 'Davies-Bouldin', 'Time (s)']\n",
    "algorithms_names = list(comparison_results.keys())\n",
    "\n",
    "# Prepare data for plotting\n",
    "data = []\n",
    "for algo in algorithms_names:\n",
    "    res = comparison_results[algo]\n",
    "    data.append([\n",
    "        res['result'].best_objective,\n",
    "        res['eval'].silhouette_score,\n",
    "        -res['eval'].davies_bouldin_index,  # Negative because lower is better\n",
    "        res['time']\n",
    "    ])\n",
    "\n",
    "data = np.array(data).T\n",
    "\n",
    "# Normalize data for radar chart (0-1 scale)\n",
    "data_norm = np.zeros_like(data)\n",
    "for i in range(len(metrics)):\n",
    "    if metrics[i] == 'Time (s)':\n",
    "        # For time, lower is better, so invert\n",
    "        data_norm[i] = 1 - (data[i] - data[i].min()) / (data[i].max() - data[i].min() + 1e-10)\n",
    "    else:\n",
    "        data_norm[i] = (data[i] - data[i].min()) / (data[i].max() - data[i].min() + 1e-10)\n",
    "\n",
    "# Create radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "data_norm = np.concatenate((data_norm, data_norm[:1]))  # Complete the circle\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Radar chart\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for idx, algo in enumerate(algorithms_names):\n",
    "    values = data_norm[:, idx]\n",
    "    ax1.plot(angles, values, 'o-', linewidth=2, label=algo, color=colors[idx])\n",
    "    ax1.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('Algorithm Performance Comparison\\n(Higher is Better)', pad=20)\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "# Bar chart for execution times\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "times = [comparison_results[algo]['time'] for algo in algorithms_names]\n",
    "bars = ax2.bar(algorithms_names, times, color=colors)\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('Algorithm Speed Comparison')\n",
    "ax2.set_xticklabels(algorithms_names, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{time:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Testing\n",
    "\n",
    "When comparing clustering solutions, it's important to test for statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empirical distribution for significance testing\n",
    "print(\"Creating empirical distribution for significance testing...\")\n",
    "\n",
    "# Generate null distribution\n",
    "null_distribution = create_switching_matrix_distribution(\n",
    "    substitution_matrix,\n",
    "    n_simulations=1000,\n",
    "    n_clusters=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calculate p-values for our solutions\n",
    "print(\"\\nStatistical Significance Testing:\")\n",
    "print(\"Algorithm | Objective | P-value | Significant?\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, res in comparison_results.items():\n",
    "    p_value = k_sm_empirical_p(\n",
    "        res['result'].best_objective,\n",
    "        null_distribution\n",
    "    )\n",
    "    significant = p_value < 0.05\n",
    "    print(f\"{name:20s} | {res['result'].best_objective:.6f} | {p_value:.4f} | {'Yes' if significant else 'No'}\")\n",
    "\n",
    "# Visualize null distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(null_distribution.values, bins=50, alpha=0.7, color='gray', edgecolor='black')\n",
    "\n",
    "# Add lines for our solutions\n",
    "for name, res in comparison_results.items():\n",
    "    plt.axvline(x=res['result'].best_objective, \n",
    "                label=name, linewidth=2, linestyle='--')\n",
    "\n",
    "plt.xlabel('Objective Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Null Distribution vs. Observed Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Parameter Tuning\n",
    "\n",
    "Let's explore how different parameters affect clustering performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_restarts': [5, 10, 20, 50],\n",
    "    'max_iterations': [50, 100, 200, 500],\n",
    "    'initialization': ['random', 'kmeans++', 'spectral']\n",
    "}\n",
    "\n",
    "# Simple grid search (subset for demonstration)\n",
    "print(\"Performing parameter tuning...\")\n",
    "results_grid = []\n",
    "\n",
    "for n_restarts in [5, 20]:\n",
    "    for max_iter in [100, 300]:\n",
    "        for init in ['random', 'kmeans++']:\n",
    "            # Run clustering\n",
    "            search = KSMLocalSearch(\n",
    "                n_clusters=4,\n",
    "                n_restarts=n_restarts,\n",
    "                max_iterations=max_iter,\n",
    "                initialization=init,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            result = search.fit(substitution_matrix)\n",
    "            eval_res = evaluator.evaluate(substitution_matrix, result.labels)\n",
    "            \n",
    "            results_grid.append({\n",
    "                'n_restarts': n_restarts,\n",
    "                'max_iterations': max_iter,\n",
    "                'initialization': init,\n",
    "                'objective': result.best_objective,\n",
    "                'silhouette': eval_res.silhouette_score,\n",
    "                'iterations_used': result.n_iterations\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(results_grid)\n",
    "\n",
    "# Find best parameters\n",
    "best_idx = df_results['objective'].idxmax()\n",
    "best_params = df_results.iloc[best_idx]\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(f\"  n_restarts: {best_params['n_restarts']}\")\n",
    "print(f\"  max_iterations: {best_params['max_iterations']}\")\n",
    "print(f\"  initialization: {best_params['initialization']}\")\n",
    "print(f\"  Objective: {best_params['objective']:.6f}\")\n",
    "print(f\"  Silhouette: {best_params['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Effect of n_restarts\n",
    "ax = axes[0, 0]\n",
    "for init in df_results['initialization'].unique():\n",
    "    data = df_results[df_results['initialization'] == init]\n",
    "    grouped = data.groupby('n_restarts')['objective'].mean()\n",
    "    ax.plot(grouped.index, grouped.values, marker='o', label=init)\n",
    "ax.set_xlabel('Number of Restarts')\n",
    "ax.set_ylabel('Average Objective')\n",
    "ax.set_title('Effect of n_restarts on Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Effect of max_iterations\n",
    "ax = axes[0, 1]\n",
    "for init in df_results['initialization'].unique():\n",
    "    data = df_results[df_results['initialization'] == init]\n",
    "    grouped = data.groupby('max_iterations')['objective'].mean()\n",
    "    ax.plot(grouped.index, grouped.values, marker='s', label=init)\n",
    "ax.set_xlabel('Max Iterations')\n",
    "ax.set_ylabel('Average Objective')\n",
    "ax.set_title('Effect of max_iterations on Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Iterations actually used\n",
    "ax = axes[1, 0]\n",
    "pivot_data = df_results.pivot_table(\n",
    "    values='iterations_used',\n",
    "    index='max_iterations',\n",
    "    columns='initialization',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot_data.plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Max Iterations Allowed')\n",
    "ax.set_ylabel('Average Iterations Used')\n",
    "ax.set_title('Convergence Speed by Initialization')\n",
    "ax.legend(title='Initialization')\n",
    "\n",
    "# 4. Performance heatmap\n",
    "ax = axes[1, 1]\n",
    "pivot_objective = df_results.pivot_table(\n",
    "    values='objective',\n",
    "    index='n_restarts',\n",
    "    columns='max_iterations',\n",
    "    aggfunc='max'\n",
    ")\n",
    "sns.heatmap(pivot_objective, annot=True, fmt='.4f', cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('Best Objective by Parameters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Constrained Clustering**:\n",
    "   - Use when you have domain knowledge about cluster requirements\n",
    "   - Helps ensure practical, actionable results\n",
    "   - May sacrifice some objective value for constraint satisfaction\n",
    "\n",
    "2. **Entropy-Based Clustering**:\n",
    "   - Balances cluster cohesion with information content\n",
    "   - Useful when cluster balance is important\n",
    "   - Can help avoid trivial solutions\n",
    "\n",
    "3. **GAP Statistic**:\n",
    "   - Provides principled way to select number of clusters\n",
    "   - Compare against elbow method for validation\n",
    "   - Computationally intensive but robust\n",
    "\n",
    "4. **Top-K Analysis**:\n",
    "   - Reveals solution landscape and stability\n",
    "   - Helps identify robust vs. unstable clusterings\n",
    "   - Useful for understanding alternative interpretations\n",
    "\n",
    "5. **Statistical Testing**:\n",
    "   - Always test significance of clustering results\n",
    "   - Use empirical distributions for p-value calculation\n",
    "   - Consider multiple testing corrections\n",
    "\n",
    "6. **Parameter Tuning**:\n",
    "   - More restarts generally improve results but increase computation\n",
    "   - Initialization method can significantly affect convergence\n",
    "   - Balance computational cost with solution quality\n",
    "\n",
    "### Recommended Workflow:\n",
    "\n",
    "1. Start with GAP statistic to determine optimal number of clusters\n",
    "2. Run multiple algorithms and compare results\n",
    "3. Use top-k analysis to assess solution stability\n",
    "4. Apply constraints if domain knowledge is available\n",
    "5. Validate results with statistical significance testing\n",
    "6. Fine-tune parameters based on specific needs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}