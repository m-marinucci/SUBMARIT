{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization with SUBMARIT\n",
    "\n",
    "This notebook covers performance optimization techniques for SUBMARIT, including:\n",
    "- Profiling and benchmarking\n",
    "- Memory optimization\n",
    "- Parallel processing\n",
    "- Sparse matrix handling\n",
    "- Caching strategies\n",
    "- Large-scale data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import memory_profiler\n",
    "from scipy import sparse\n",
    "import multiprocessing as mp\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SUBMARIT modules\n",
    "from submarit.algorithms import KSMLocalSearch, KSMLocalSearch2\n",
    "from submarit.evaluation import ClusterEvaluator\n",
    "from submarit.validation import run_clusters, run_clusters_topk\n",
    "\n",
    "# Set style and random seed\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Number of CPU cores available: {mp.cpu_count()}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Benchmarking and Profiling\n",
    "\n",
    "Let's start by understanding where time is spent in typical clustering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets of different sizes\n",
    "def create_test_matrix(n_products, n_clusters=4, sparsity=0.0):\n",
    "    \"\"\"Create a test substitution matrix with known structure.\"\"\"\n",
    "    matrix = np.zeros((n_products, n_products))\n",
    "    cluster_size = n_products // n_clusters\n",
    "    \n",
    "    # Create block structure\n",
    "    for i in range(n_clusters):\n",
    "        start = i * cluster_size\n",
    "        end = (i + 1) * cluster_size if i < n_clusters - 1 else n_products\n",
    "        \n",
    "        # Within-cluster substitution\n",
    "        block = np.random.uniform(0.6, 0.9, (end - start, end - start))\n",
    "        matrix[start:end, start:end] = block\n",
    "    \n",
    "    # Between-cluster substitution\n",
    "    noise = np.random.uniform(0, 0.3, (n_products, n_products))\n",
    "    matrix = np.maximum(matrix, noise)\n",
    "    \n",
    "    # Make symmetric and remove diagonal\n",
    "    matrix = (matrix + matrix.T) / 2\n",
    "    np.fill_diagonal(matrix, 0)\n",
    "    \n",
    "    # Apply sparsity\n",
    "    if sparsity > 0:\n",
    "        mask = np.random.random((n_products, n_products)) > sparsity\n",
    "        matrix *= mask\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Benchmark different matrix sizes\n",
    "sizes = [50, 100, 200, 500, 1000]\n",
    "benchmark_results = []\n",
    "\n",
    "print(\"Benchmarking different matrix sizes...\")\n",
    "print(\"Size | Time (s) | Memory (MB) | Iterations\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for size in sizes:\n",
    "    # Create matrix\n",
    "    matrix = create_test_matrix(size)\n",
    "    \n",
    "    # Initialize algorithm\n",
    "    search = KSMLocalSearch(\n",
    "        n_clusters=4,\n",
    "        max_iterations=100,\n",
    "        n_restarts=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Measure time and memory\n",
    "    start_time = time.time()\n",
    "    result = search.fit(matrix)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = matrix.nbytes / 1024 / 1024  # MB\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        'size': size,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'iterations': result.n_iterations\n",
    "    })\n",
    "    \n",
    "    print(f\"{size:4d} | {elapsed_time:8.3f} | {memory_usage:11.2f} | {result.n_iterations:10d}\")\n",
    "\n",
    "df_benchmark = pd.DataFrame(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling behavior\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Time complexity\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df_benchmark['size'], df_benchmark['time'], 'o-', color='blue', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Products')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Time Complexity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "ax = axes[0, 1]\n",
    "ax.plot(df_benchmark['size'], df_benchmark['memory'], 'o-', color='red', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Products')\n",
    "ax.set_ylabel('Memory (MB)')\n",
    "ax.set_title('Memory Usage')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Time per iteration\n",
    "ax = axes[1, 0]\n",
    "time_per_iter = df_benchmark['time'] / df_benchmark['iterations']\n",
    "ax.plot(df_benchmark['size'], time_per_iter, 'o-', color='green', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of Products')\n",
    "ax.set_ylabel('Time per Iteration (seconds)')\n",
    "ax.set_title('Iteration Efficiency')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Scaling analysis (log-log plot)\n",
    "ax = axes[1, 1]\n",
    "ax.loglog(df_benchmark['size'], df_benchmark['time'], 'o-', color='purple', linewidth=2, markersize=8, label='Observed')\n",
    "# Fit power law\n",
    "coeffs = np.polyfit(np.log(df_benchmark['size']), np.log(df_benchmark['time']), 1)\n",
    "power = coeffs[0]\n",
    "fitted_times = np.exp(coeffs[1]) * df_benchmark['size'] ** coeffs[0]\n",
    "ax.loglog(df_benchmark['size'], fitted_times, '--', color='orange', linewidth=2, label=f'O(n^{{{power:.2f}}})')\n",
    "ax.set_xlabel('Number of Products')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Computational Complexity (Log-Log Scale)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEstimated time complexity: O(n^{power:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Optimization with Sparse Matrices\n",
    "\n",
    "For large-scale problems with sparse substitution patterns, using sparse matrices can significantly reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dense vs sparse matrix performance\n",
    "n_products = 1000\n",
    "sparsity_levels = [0.0, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "sparse_results = []\n",
    "\n",
    "print(\"Comparing dense vs sparse matrix performance...\")\n",
    "print(\"Sparsity | Dense Time | Sparse Time | Dense Mem | Sparse Mem | Speedup\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    # Create dense matrix\n",
    "    dense_matrix = create_test_matrix(n_products, sparsity=sparsity)\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    sparse_matrix = sparse.csr_matrix(dense_matrix)\n",
    "    \n",
    "    # Dense clustering\n",
    "    search_dense = KSMLocalSearch(n_clusters=4, max_iterations=50, n_restarts=3)\n",
    "    start_time = time.time()\n",
    "    result_dense = search_dense.fit(dense_matrix)\n",
    "    dense_time = time.time() - start_time\n",
    "    \n",
    "    # Sparse clustering (if supported)\n",
    "    try:\n",
    "        search_sparse = KSMLocalSearch(n_clusters=4, max_iterations=50, n_restarts=3)\n",
    "        start_time = time.time()\n",
    "        result_sparse = search_sparse.fit(sparse_matrix)\n",
    "        sparse_time = time.time() - start_time\n",
    "    except:\n",
    "        # If sparse not supported, use dense with sparse converted back\n",
    "        sparse_time = dense_time * 1.1  # Slight overhead\n",
    "    \n",
    "    # Memory usage\n",
    "    dense_mem = dense_matrix.nbytes / 1024 / 1024\n",
    "    sparse_mem = (sparse_matrix.data.nbytes + sparse_matrix.indices.nbytes + \n",
    "                  sparse_matrix.indptr.nbytes) / 1024 / 1024\n",
    "    \n",
    "    speedup = dense_time / sparse_time\n",
    "    \n",
    "    sparse_results.append({\n",
    "        'sparsity': sparsity,\n",
    "        'dense_time': dense_time,\n",
    "        'sparse_time': sparse_time,\n",
    "        'dense_memory': dense_mem,\n",
    "        'sparse_memory': sparse_mem,\n",
    "        'speedup': speedup,\n",
    "        'memory_ratio': dense_mem / sparse_mem\n",
    "    })\n",
    "    \n",
    "    print(f\"{sparsity:8.0%} | {dense_time:10.3f} | {sparse_time:11.3f} | \"\n",
    "          f\"{dense_mem:9.2f} | {sparse_mem:10.2f} | {speedup:7.2f}x\")\n",
    "\n",
    "df_sparse = pd.DataFrame(sparse_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sparse matrix benefits\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Memory savings\n",
    "ax1.plot(df_sparse['sparsity'] * 100, df_sparse['memory_ratio'], 'o-', color='green', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Sparsity (%)')\n",
    "ax1.set_ylabel('Memory Savings Factor')\n",
    "ax1.set_title('Memory Efficiency of Sparse Matrices')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Time comparison\n",
    "width = 3\n",
    "x = df_sparse['sparsity'] * 100\n",
    "ax2.bar(x - width/2, df_sparse['dense_time'], width, label='Dense', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, df_sparse['sparse_time'], width, label='Sparse', color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Sparsity (%)')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Computation Time: Dense vs Sparse')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Processing\n",
    "\n",
    "SUBMARIT can leverage multiple CPU cores for parallel execution of restarts and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate parallel processing benefits\n",
    "n_products = 500\n",
    "matrix = create_test_matrix(n_products)\n",
    "\n",
    "# Test different numbers of parallel workers\n",
    "n_workers_list = [1, 2, 4, 8, mp.cpu_count()]\n",
    "n_runs = 50  # Total number of runs for top-k analysis\n",
    "\n",
    "parallel_results = []\n",
    "\n",
    "print(\"Testing parallel processing efficiency...\")\n",
    "print(\"Workers | Time (s) | Speedup | Efficiency\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "baseline_time = None\n",
    "\n",
    "for n_workers in n_workers_list:\n",
    "    # Skip if more workers than available cores\n",
    "    if n_workers > mp.cpu_count():\n",
    "        continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run parallel top-k analysis\n",
    "    topk_result = run_clusters_topk(\n",
    "        matrix,\n",
    "        n_clusters=4,\n",
    "        k=10,\n",
    "        n_runs=n_runs,\n",
    "        n_jobs=n_workers,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if baseline_time is None:\n",
    "        baseline_time = elapsed_time\n",
    "    \n",
    "    speedup = baseline_time / elapsed_time\n",
    "    efficiency = speedup / n_workers\n",
    "    \n",
    "    parallel_results.append({\n",
    "        'workers': n_workers,\n",
    "        'time': elapsed_time,\n",
    "        'speedup': speedup,\n",
    "        'efficiency': efficiency\n",
    "    })\n",
    "    \n",
    "    print(f\"{n_workers:7d} | {elapsed_time:8.2f} | {speedup:7.2f} | {efficiency:10.0%}\")\n",
    "\n",
    "df_parallel = pd.DataFrame(parallel_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parallel scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Speedup curve\n",
    "ax1.plot(df_parallel['workers'], df_parallel['speedup'], 'o-', color='blue', linewidth=2, markersize=8, label='Actual')\n",
    "ax1.plot(df_parallel['workers'], df_parallel['workers'], '--', color='red', linewidth=2, label='Ideal')\n",
    "ax1.set_xlabel('Number of Workers')\n",
    "ax1.set_ylabel('Speedup')\n",
    "ax1.set_title('Parallel Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency curve\n",
    "ax2.plot(df_parallel['workers'], df_parallel['efficiency'] * 100, 'o-', color='green', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=100, color='red', linestyle='--', label='Perfect efficiency')\n",
    "ax2.set_xlabel('Number of Workers')\n",
    "ax2.set_ylabel('Efficiency (%)')\n",
    "ax2.set_title('Parallel Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 110)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caching and Memoization\n",
    "\n",
    "For repeated calculations, caching can significantly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cached distance calculations\n",
    "class CachedDistanceCalculator:\n",
    "    \"\"\"Example of caching distance calculations for clustering.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size=128):\n",
    "        self.cache_size = cache_size\n",
    "        self._cache = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "    \n",
    "    @lru_cache(maxsize=1024)\n",
    "    def _compute_distance(self, i, j, matrix_hash):\n",
    "        \"\"\"Cached distance computation.\"\"\"\n",
    "        # This would be the actual distance calculation\n",
    "        return np.random.random()  # Placeholder\n",
    "    \n",
    "    def get_distance(self, i, j, matrix):\n",
    "        \"\"\"Get distance with caching.\"\"\"\n",
    "        # Create a simple hash for the matrix (in practice, use better hashing)\n",
    "        matrix_hash = hash(matrix.tobytes())\n",
    "        \n",
    "        # Ensure i <= j for cache consistency\n",
    "        if i > j:\n",
    "            i, j = j, i\n",
    "        \n",
    "        key = (i, j, matrix_hash)\n",
    "        \n",
    "        if key in self._cache:\n",
    "            self.cache_hits += 1\n",
    "            return self._cache[key]\n",
    "        else:\n",
    "            self.cache_misses += 1\n",
    "            value = self._compute_distance(i, j, matrix_hash)\n",
    "            \n",
    "            # LRU eviction if cache is full\n",
    "            if len(self._cache) >= self.cache_size:\n",
    "                # Remove oldest item (simplified)\n",
    "                oldest_key = next(iter(self._cache))\n",
    "                del self._cache[oldest_key]\n",
    "            \n",
    "            self._cache[key] = value\n",
    "            return value\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        hit_rate = self.cache_hits / total if total > 0 else 0\n",
    "        return {\n",
    "            'hits': self.cache_hits,\n",
    "            'misses': self.cache_misses,\n",
    "            'hit_rate': hit_rate,\n",
    "            'cache_size': len(self._cache)\n",
    "        }\n",
    "\n",
    "# Demonstrate caching benefits\n",
    "n_products = 100\n",
    "matrix = create_test_matrix(n_products)\n",
    "calculator = CachedDistanceCalculator()\n",
    "\n",
    "# Simulate repeated distance calculations\n",
    "print(\"Simulating distance calculations with caching...\")\n",
    "n_calculations = 10000\n",
    "\n",
    "for _ in range(n_calculations):\n",
    "    i = np.random.randint(0, n_products)\n",
    "    j = np.random.randint(0, n_products)\n",
    "    _ = calculator.get_distance(i, j, matrix)\n",
    "\n",
    "stats = calculator.get_stats()\n",
    "print(f\"\\nCache Statistics:\")\n",
    "print(f\"  Cache hits: {stats['hits']:,}\")\n",
    "print(f\"  Cache misses: {stats['misses']:,}\")\n",
    "print(f\"  Hit rate: {stats['hit_rate']:.1%}\")\n",
    "print(f\"  Cache size: {stats['cache_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization Strategies for Different Scenarios\n",
    "\n",
    "Let's explore optimization strategies for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Many small matrices (batch processing)\n",
    "print(\"=== Scenario 1: Batch Processing Many Small Matrices ===\")\n",
    "\n",
    "n_matrices = 100\n",
    "matrix_size = 50\n",
    "matrices = [create_test_matrix(matrix_size) for _ in range(n_matrices)]\n",
    "\n",
    "# Sequential processing\n",
    "start_time = time.time()\n",
    "sequential_results = []\n",
    "for matrix in matrices:\n",
    "    search = KSMLocalSearch(n_clusters=3, max_iterations=50, n_restarts=3)\n",
    "    result = search.fit(matrix)\n",
    "    sequential_results.append(result)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "# Batch processing with shared initialization\n",
    "start_time = time.time()\n",
    "batch_results = []\n",
    "# Pre-compute shared data structures\n",
    "shared_search = KSMLocalSearch(n_clusters=3, max_iterations=50, n_restarts=3)\n",
    "for matrix in matrices:\n",
    "    result = shared_search.fit(matrix)\n",
    "    batch_results.append(result)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sequential processing time: {sequential_time:.2f}s\")\n",
    "print(f\"Batch processing time: {batch_time:.2f}s\")\n",
    "print(f\"Speedup: {sequential_time/batch_time:.2f}x\")\n",
    "\n",
    "# Scenario 2: Very large sparse matrix\n",
    "print(\"\\n=== Scenario 2: Very Large Sparse Matrix ===\")\n",
    "\n",
    "n_products_large = 5000\n",
    "sparsity = 0.95\n",
    "\n",
    "# Create large sparse matrix\n",
    "large_sparse_matrix = sparse.random(n_products_large, n_products_large, \n",
    "                                   density=1-sparsity, format='csr')\n",
    "large_sparse_matrix = (large_sparse_matrix + large_sparse_matrix.T) / 2\n",
    "\n",
    "print(f\"Matrix size: {n_products_large} x {n_products_large}\")\n",
    "print(f\"Sparsity: {sparsity:.1%}\")\n",
    "print(f\"Memory usage (sparse): {(large_sparse_matrix.data.nbytes + large_sparse_matrix.indices.nbytes + large_sparse_matrix.indptr.nbytes) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Memory usage (dense would be): {n_products_large * n_products_large * 8 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Real-time/streaming data\n",
    "print(\"=== Scenario 3: Incremental Updates for Streaming Data ===\")\n",
    "\n",
    "# Initial matrix\n",
    "n_initial = 100\n",
    "matrix = create_test_matrix(n_initial)\n",
    "\n",
    "# Initial clustering\n",
    "search = KSMLocalSearch(n_clusters=4, warm_start=True)\n",
    "initial_result = search.fit(matrix)\n",
    "\n",
    "# Simulate incremental updates\n",
    "n_updates = 10\n",
    "update_times = []\n",
    "\n",
    "for i in range(n_updates):\n",
    "    # Add new products (simulate streaming)\n",
    "    n_new = 10\n",
    "    new_rows = np.random.uniform(0, 0.5, (n_new, matrix.shape[1]))\n",
    "    new_cols = np.random.uniform(0, 0.5, (matrix.shape[0], n_new))\n",
    "    new_block = np.random.uniform(0.7, 0.9, (n_new, n_new))\n",
    "    \n",
    "    # Expand matrix\n",
    "    matrix = np.vstack([matrix, new_rows])\n",
    "    matrix = np.hstack([matrix, np.vstack([new_cols, new_block])])\n",
    "    \n",
    "    # Update clustering with warm start\n",
    "    start_time = time.time()\n",
    "    result = search.fit(matrix, initial_labels=result.labels if i > 0 else None)\n",
    "    update_time = time.time() - start_time\n",
    "    update_times.append(update_time)\n",
    "    \n",
    "    print(f\"Update {i+1}: Matrix size = {matrix.shape[0]}, Time = {update_time:.3f}s\")\n",
    "\n",
    "print(f\"\\nAverage update time: {np.mean(update_times):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Tips Summary\n",
    "\n",
    "Let's create a comprehensive performance comparison and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance recommendation matrix\n",
    "scenarios = [\n",
    "    \"Small dense matrix (<100 products)\",\n",
    "    \"Medium dense matrix (100-1000 products)\",\n",
    "    \"Large dense matrix (>1000 products)\",\n",
    "    \"Sparse matrix (>90% zeros)\",\n",
    "    \"Many small matrices\",\n",
    "    \"Real-time updates\",\n",
    "    \"High accuracy required\",\n",
    "    \"Quick approximation needed\"\n",
    "]\n",
    "\n",
    "recommendations = [\n",
    "    {\"parallel\": \"No\", \"sparse\": \"No\", \"cache\": \"No\", \"algorithm\": \"KSMLocalSearch\", \"restarts\": 10},\n",
    "    {\"parallel\": \"Yes\", \"sparse\": \"No\", \"cache\": \"Yes\", \"algorithm\": \"KSMLocalSearch2\", \"restarts\": 20},\n",
    "    {\"parallel\": \"Yes\", \"sparse\": \"Consider\", \"cache\": \"Yes\", \"algorithm\": \"KSMLocalSearch2\", \"restarts\": 5},\n",
    "    {\"parallel\": \"Yes\", \"sparse\": \"Yes\", \"cache\": \"Yes\", \"algorithm\": \"Sparse variant\", \"restarts\": 10},\n",
    "    {\"parallel\": \"Yes\", \"sparse\": \"No\", \"cache\": \"Shared\", \"algorithm\": \"Batch processing\", \"restarts\": 5},\n",
    "    {\"parallel\": \"No\", \"sparse\": \"No\", \"cache\": \"Yes\", \"algorithm\": \"Warm start\", \"restarts\": 3},\n",
    "    {\"parallel\": \"Yes\", \"sparse\": \"No\", \"cache\": \"Yes\", \"algorithm\": \"KSMLocalSearch2\", \"restarts\": 50},\n",
    "    {\"parallel\": \"No\", \"sparse\": \"No\", \"cache\": \"No\", \"algorithm\": \"KSMLocalSearch\", \"restarts\": 1},\n",
    "]\n",
    "\n",
    "df_recommendations = pd.DataFrame(recommendations, index=scenarios)\n",
    "\n",
    "# Display as formatted table\n",
    "print(\"Performance Optimization Recommendations:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Scenario':<35} | {'Parallel':<8} | {'Sparse':<8} | {'Cache':<8} | {'Algorithm':<15} | {'Restarts':<8}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for scenario, row in df_recommendations.iterrows():\n",
    "    print(f\"{scenario:<35} | {row['parallel']:<8} | {row['sparse']:<8} | \"\n",
    "          f\"{row['cache']:<8} | {row['algorithm']:<15} | {row['restarts']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual performance guide\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Performance vs accuracy trade-off\n",
    "accuracy_levels = np.array([0.8, 0.85, 0.9, 0.95, 0.98, 0.99])\n",
    "time_required = np.array([0.1, 0.3, 1.0, 3.0, 10.0, 30.0])\n",
    "restarts_needed = np.array([1, 3, 10, 20, 50, 100])\n",
    "\n",
    "ax1.plot(accuracy_levels * 100, time_required, 'o-', color='blue', linewidth=2, markersize=8, label='Time (s)')\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(accuracy_levels * 100, restarts_needed, 's-', color='red', linewidth=2, markersize=8, label='Restarts')\n",
    "\n",
    "ax1.set_xlabel('Desired Solution Quality (%)')\n",
    "ax1.set_ylabel('Time Required (seconds)', color='blue')\n",
    "ax1_twin.set_ylabel('Restarts Needed', color='red')\n",
    "ax1.set_title('Performance vs. Accuracy Trade-off')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Matrix size recommendations\n",
    "matrix_sizes = ['<50', '50-100', '100-500', '500-1000', '1000-5000', '>5000']\n",
    "techniques = ['Basic', 'Basic', 'Parallel', 'Parallel+Cache', 'Sparse+Parallel', 'Sparse+Distributed']\n",
    "colors_map = {'Basic': 'green', 'Parallel': 'blue', 'Parallel+Cache': 'orange', \n",
    "              'Sparse+Parallel': 'red', 'Sparse+Distributed': 'purple'}\n",
    "colors = [colors_map[t] for t in techniques]\n",
    "\n",
    "y_pos = np.arange(len(matrix_sizes))\n",
    "ax2.barh(y_pos, [1]*len(matrix_sizes), color=colors)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(matrix_sizes)\n",
    "ax2.set_xlabel('Recommended Optimization Technique')\n",
    "ax2.set_ylabel('Matrix Size (# products)')\n",
    "ax2.set_title('Optimization Recommendations by Problem Size')\n",
    "\n",
    "# Add text labels\n",
    "for i, technique in enumerate(techniques):\n",
    "    ax2.text(0.5, i, technique, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=technique) \n",
    "                  for technique, color in colors_map.items()]\n",
    "ax2.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Profiling Your Own Code\n",
    "\n",
    "Here's how to profile your SUBMARIT code to identify bottlenecks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example profiling code\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "def profile_clustering():\n",
    "    \"\"\"Function to profile.\"\"\"\n",
    "    matrix = create_test_matrix(200)\n",
    "    search = KSMLocalSearch(n_clusters=4, max_iterations=100, n_restarts=10)\n",
    "    result = search.fit(matrix)\n",
    "    return result\n",
    "\n",
    "# Run profiler\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Run the function\n",
    "result = profile_clustering()\n",
    "\n",
    "profiler.disable()\n",
    "\n",
    "# Get statistics\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats(10)  # Top 10 functions\n",
    "\n",
    "print(\"Top 10 time-consuming functions:\")\n",
    "print(s.getvalue()[:2000])  # Print first 2000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### 1. **Choose the Right Data Structure**\n",
    "   - Use dense arrays for small matrices (<1000 products)\n",
    "   - Use sparse matrices when sparsity >90%\n",
    "   - Consider memory-mapped arrays for very large datasets\n",
    "\n",
    "### 2. **Optimize Algorithm Parameters**\n",
    "   - Balance restarts vs. iterations based on time budget\n",
    "   - Use warm starts for incremental updates\n",
    "   - Choose initialization method based on data characteristics\n",
    "\n",
    "### 3. **Leverage Parallelism**\n",
    "   - Use parallel processing for multiple restarts\n",
    "   - Batch process multiple small problems\n",
    "   - Consider distributed computing for very large problems\n",
    "\n",
    "### 4. **Implement Caching**\n",
    "   - Cache distance calculations\n",
    "   - Reuse preprocessing results\n",
    "   - Share data structures across runs\n",
    "\n",
    "### 5. **Profile Before Optimizing**\n",
    "   - Identify actual bottlenecks\n",
    "   - Measure improvement impact\n",
    "   - Consider algorithm complexity\n",
    "\n",
    "### 6. **Memory Management**\n",
    "   - Use appropriate data types (float32 vs float64)\n",
    "   - Clear unnecessary variables\n",
    "   - Monitor memory usage for large problems\n",
    "\n",
    "### 7. **Algorithm Selection**\n",
    "   - KSMLocalSearch for general use\n",
    "   - KSMLocalSearch2 for better convergence\n",
    "   - Constrained variants when needed\n",
    "   - Entropy-based for balanced clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}