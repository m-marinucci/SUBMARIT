{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization with SUBMARIT\n",
    "\n",
    "This notebook demonstrates techniques for optimizing SUBMARIT performance with large datasets.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Memory-Efficient Techniques](#memory)\n",
    "2. [Computational Optimization](#computation)\n",
    "3. [Parallel and Distributed Processing](#parallel)\n",
    "4. [Approximate Methods](#approximate)\n",
    "5. [Benchmarking and Profiling](#benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from memory_profiler import memory_usage\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# SUBMARIT imports\n",
    "from submarit.core import (\n",
    "    create_substitution_matrix,\n",
    "    create_sparse_substitution_matrix\n",
    ")\n",
    "from submarit.algorithms import LocalSearch, MiniBatchLocalSearch\n",
    "from submarit.utils import Timer\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Performance optimization notebook ready!\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory-Efficient Techniques <a id='memory'></a>\n",
    "\n",
    "Strategies for reducing memory usage with large substitution matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets of increasing size\n",
    "sizes = [100, 500, 1000, 2000, 5000]\n",
    "n_features = 20\n",
    "\n",
    "memory_results = []\n",
    "\n",
    "for n in sizes:\n",
    "    print(f\"\\nProcessing {n} products...\")\n",
    "    X = np.random.randn(n, n_features).astype(np.float32)  # Use float32 to save memory\n",
    "    \n",
    "    # Dense matrix memory\n",
    "    dense_memory = (n * n * 4) / 1e6  # 4 bytes per float32\n",
    "    \n",
    "    # Sparse matrix estimation (keep top 10%)\n",
    "    sparsity = 0.9\n",
    "    sparse_memory = (n * n * (1 - sparsity) * 12) / 1e6  # ~12 bytes per entry\n",
    "    \n",
    "    memory_results.append({\n",
    "        'n_products': n,\n",
    "        'dense_mb': dense_memory,\n",
    "        'sparse_mb': sparse_memory,\n",
    "        'ratio': dense_memory / sparse_memory\n",
    "    })\n",
    "    \n",
    "    print(f\"Dense: {dense_memory:.1f} MB, Sparse: {sparse_memory:.1f} MB, Ratio: {dense_memory/sparse_memory:.1f}x\")\n",
    "\n",
    "# Visualize memory usage\n",
    "df_memory = pd.DataFrame(memory_results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(df_memory['n_products'], df_memory['dense_mb'], 'o-', label='Dense', linewidth=2)\n",
    "ax1.plot(df_memory['n_products'], df_memory['sparse_mb'], 's-', label='Sparse (10%)', linewidth=2)\n",
    "ax1.set_xlabel('Number of Products')\n",
    "ax1.set_ylabel('Memory (MB)')\n",
    "ax1.set_title('Memory Usage Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2.bar(df_memory['n_products'].astype(str), df_memory['ratio'])\n",
    "ax2.set_xlabel('Number of Products')\n",
    "ax2.set_ylabel('Memory Savings Ratio')\n",
    "ax2.set_title('Sparse vs Dense Memory Savings')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Sparse matrix creation and usage\n",
    "n_demo = 1000\n",
    "X_demo = np.random.randn(n_demo, n_features)\n",
    "\n",
    "print(\"Creating substitution matrices...\")\n",
    "\n",
    "# Dense matrix\n",
    "start = time.time()\n",
    "S_dense = create_substitution_matrix(X_demo, metric='euclidean')\n",
    "time_dense = time.time() - start\n",
    "mem_dense = S_dense.nbytes / 1e6\n",
    "\n",
    "# Sparse matrix (keep top 10%)\n",
    "start = time.time()\n",
    "S_sparse = create_sparse_substitution_matrix(X_demo, threshold=0.1, metric='euclidean')\n",
    "time_sparse = time.time() - start\n",
    "mem_sparse = (S_sparse.data.nbytes + S_sparse.indices.nbytes + S_sparse.indptr.nbytes) / 1e6\n",
    "\n",
    "print(f\"\\nDense matrix:\")\n",
    "print(f\"  Creation time: {time_dense:.2f}s\")\n",
    "print(f\"  Memory usage: {mem_dense:.1f} MB\")\n",
    "\n",
    "print(f\"\\nSparse matrix:\")\n",
    "print(f\"  Creation time: {time_sparse:.2f}s\")\n",
    "print(f\"  Memory usage: {mem_sparse:.1f} MB\")\n",
    "print(f\"  Sparsity: {1 - S_sparse.nnz / (n_demo**2):.1%}\")\n",
    "print(f\"  Memory reduction: {(1 - mem_sparse/mem_dense)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-mapped arrays for very large datasets\n",
    "def create_mmap_substitution_matrix(X, filename='submatrix.dat', dtype=np.float32):\n",
    "    \"\"\"Create memory-mapped substitution matrix.\"\"\"\n",
    "    n = len(X)\n",
    "    \n",
    "    # Create memory-mapped array\n",
    "    S = np.memmap(filename, dtype=dtype, mode='w+', shape=(n, n))\n",
    "    \n",
    "    # Compute in chunks to avoid memory overflow\n",
    "    chunk_size = min(100, n)\n",
    "    \n",
    "    for i in range(0, n, chunk_size):\n",
    "        end_i = min(i + chunk_size, n)\n",
    "        for j in range(0, n, chunk_size):\n",
    "            end_j = min(j + chunk_size, n)\n",
    "            \n",
    "            # Compute chunk\n",
    "            dists = pdist(np.vstack([X[i:end_i], X[j:end_j]]), metric='euclidean')\n",
    "            dist_matrix = squareform(dists)\n",
    "            \n",
    "            # Extract relevant part\n",
    "            S[i:end_i, j:end_j] = dist_matrix[:end_i-i, end_i-i:end_i-i+end_j-j]\n",
    "    \n",
    "    # Ensure symmetry\n",
    "    S[:] = np.maximum(S, S.T)\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Demo with smaller dataset\n",
    "n_mmap = 500\n",
    "X_mmap = np.random.randn(n_mmap, n_features)\n",
    "\n",
    "print(\"Creating memory-mapped substitution matrix...\")\n",
    "S_mmap = create_mmap_substitution_matrix(X_mmap, 'demo_submatrix.dat')\n",
    "\n",
    "print(f\"Memory-mapped matrix created\")\n",
    "print(f\"File size: {os.path.getsize('demo_submatrix.dat') / 1e6:.1f} MB\")\n",
    "print(f\"Can handle much larger datasets without loading into RAM\")\n",
    "\n",
    "# Clean up\n",
    "del S_mmap\n",
    "os.remove('demo_submatrix.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Optimization <a id='computation'></a>\n",
    "\n",
    "Speed up computations using various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization comparison\n",
    "n_test = 1000\n",
    "X_test = np.random.randn(n_test, n_features)\n",
    "\n",
    "# Naive implementation with loops\n",
    "def naive_substitution_matrix(X):\n",
    "    n = len(X)\n",
    "    S = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.sqrt(np.sum((X[i] - X[j])**2))\n",
    "            S[i, j] = S[j, i] = dist\n",
    "    return S\n",
    "\n",
    "# Vectorized implementation\n",
    "def vectorized_substitution_matrix(X):\n",
    "    return squareform(pdist(X, metric='euclidean'))\n",
    "\n",
    "# Time comparison (with smaller dataset for naive)\n",
    "n_small = 200\n",
    "X_small = X_test[:n_small]\n",
    "\n",
    "print(\"Timing comparison (200 products):\")\n",
    "start = time.time()\n",
    "S_naive = naive_substitution_matrix(X_small)\n",
    "time_naive = time.time() - start\n",
    "print(f\"Naive (loops): {time_naive:.3f}s\")\n",
    "\n",
    "start = time.time()\n",
    "S_vectorized = vectorized_substitution_matrix(X_small)\n",
    "time_vectorized = time.time() - start\n",
    "print(f\"Vectorized: {time_vectorized:.3f}s\")\n",
    "print(f\"Speedup: {time_naive/time_vectorized:.1f}x\")\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"\\nResults match: {np.allclose(S_naive, S_vectorized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numba for JIT compilation\n",
    "try:\n",
    "    from numba import jit, prange\n",
    "    \n",
    "    @jit(nopython=True, parallel=True)\n",
    "    def numba_euclidean_distances(X):\n",
    "        n = X.shape[0]\n",
    "        distances = np.zeros((n, n))\n",
    "        \n",
    "        for i in prange(n):\n",
    "            for j in range(i+1, n):\n",
    "                dist = 0.0\n",
    "                for k in range(X.shape[1]):\n",
    "                    dist += (X[i, k] - X[j, k])**2\n",
    "                dist = np.sqrt(dist)\n",
    "                distances[i, j] = distances[j, i] = dist\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    # Warm-up JIT\n",
    "    _ = numba_euclidean_distances(X_small[:10])\n",
    "    \n",
    "    # Time comparison\n",
    "    print(\"\\nNumba JIT compilation:\")\n",
    "    start = time.time()\n",
    "    S_numba = numba_euclidean_distances(X_small)\n",
    "    time_numba = time.time() - start\n",
    "    print(f\"Numba (parallel): {time_numba:.3f}s\")\n",
    "    print(f\"Speedup vs naive: {time_naive/time_numba:.1f}x\")\n",
    "    print(f\"Results match: {np.allclose(S_numba, S_vectorized)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Numba not installed. Install with: pip install numba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Local Search implementation\n",
    "class OptimizedLocalSearch:\n",
    "    \"\"\"Local Search with performance optimizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, max_iter=100, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit_predict(self, S):\n",
    "        n = len(S)\n",
    "        \n",
    "        # Initialize clusters\n",
    "        clusters = np.random.randint(0, self.n_clusters, n)\n",
    "        \n",
    "        # Pre-allocate arrays\n",
    "        cluster_costs = np.zeros(self.n_clusters)\n",
    "        best_improvement = np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            changed = False\n",
    "            total_improvement = 0\n",
    "            \n",
    "            # Randomize order for better convergence\n",
    "            order = np.random.permutation(n)\n",
    "            \n",
    "            for i in order:\n",
    "                old_cluster = clusters[i]\n",
    "                \n",
    "                # Vectorized cost computation\n",
    "                for k in range(self.n_clusters):\n",
    "                    mask = clusters == k\n",
    "                    if k != old_cluster:\n",
    "                        mask[i] = True\n",
    "                    else:\n",
    "                        mask[i] = False\n",
    "                    \n",
    "                    cluster_costs[k] = S[i, mask].sum() if mask.any() else 0\n",
    "                \n",
    "                # Find best cluster\n",
    "                best_cluster = np.argmin(cluster_costs)\n",
    "                \n",
    "                if best_cluster != old_cluster:\n",
    "                    improvement = cluster_costs[old_cluster] - cluster_costs[best_cluster]\n",
    "                    if improvement > 0:\n",
    "                        clusters[i] = best_cluster\n",
    "                        changed = True\n",
    "                        total_improvement += improvement\n",
    "            \n",
    "            if not changed or total_improvement < self.tol:\n",
    "                break\n",
    "        \n",
    "        self.n_iter_ = iteration + 1\n",
    "        return clusters\n",
    "\n",
    "# Compare with standard implementation\n",
    "S_test = S_dense[:500, :500]\n",
    "\n",
    "print(\"Algorithm comparison (500 products, 5 clusters):\")\n",
    "\n",
    "# Standard\n",
    "start = time.time()\n",
    "ls_standard = LocalSearch(n_clusters=5, max_iter=50, n_restarts=1)\n",
    "clusters_standard = ls_standard.fit_predict(S_test)\n",
    "time_standard = time.time() - start\n",
    "\n",
    "# Optimized\n",
    "start = time.time()\n",
    "ls_optimized = OptimizedLocalSearch(n_clusters=5, max_iter=50)\n",
    "clusters_optimized = ls_optimized.fit_predict(S_test)\n",
    "time_optimized = time.time() - start\n",
    "\n",
    "print(f\"Standard: {time_standard:.3f}s\")\n",
    "print(f\"Optimized: {time_optimized:.3f}s\")\n",
    "print(f\"Speedup: {time_standard/time_optimized:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel and Distributed Processing <a id='parallel'></a>\n",
    "\n",
    "Leverage multiple cores and distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Parallel distance computation\n",
    "def compute_distance_chunk(X, i_start, i_end, j_start, j_end):\n",
    "    \"\"\"Compute a chunk of the distance matrix.\"\"\"\n",
    "    chunk = np.zeros((i_end - i_start, j_end - j_start))\n",
    "    \n",
    "    for i in range(i_end - i_start):\n",
    "        for j in range(j_end - j_start):\n",
    "            if i_start + i != j_start + j:  # Skip diagonal\n",
    "                chunk[i, j] = np.linalg.norm(X[i_start + i] - X[j_start + j])\n",
    "    \n",
    "    return i_start, i_end, j_start, j_end, chunk\n",
    "\n",
    "def parallel_distance_matrix(X, n_jobs=-1, chunk_size=100):\n",
    "    \"\"\"Compute distance matrix in parallel.\"\"\"\n",
    "    n = len(X)\n",
    "    S = np.zeros((n, n))\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    for i in range(0, n, chunk_size):\n",
    "        for j in range(i, n, chunk_size):  # Upper triangle only\n",
    "            chunks.append((X, i, min(i + chunk_size, n), \n",
    "                          j, min(j + chunk_size, n)))\n",
    "    \n",
    "    # Process in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_distance_chunk)(*chunk) for chunk in chunks\n",
    "    )\n",
    "    \n",
    "    # Assemble results\n",
    "    for i_start, i_end, j_start, j_end, chunk in results:\n",
    "        S[i_start:i_end, j_start:j_end] = chunk\n",
    "        if i_start != j_start:  # Mirror for lower triangle\n",
    "            S[j_start:j_end, i_start:i_end] = chunk.T\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Test parallel computation\n",
    "n_parallel = 500\n",
    "X_parallel = np.random.randn(n_parallel, n_features)\n",
    "\n",
    "print(f\"Computing distance matrix for {n_parallel} products...\")\n",
    "print(f\"Available cores: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# Serial\n",
    "start = time.time()\n",
    "S_serial = squareform(pdist(X_parallel))\n",
    "time_serial = time.time() - start\n",
    "\n",
    "# Parallel\n",
    "start = time.time()\n",
    "S_parallel = parallel_distance_matrix(X_parallel, n_jobs=-1, chunk_size=100)\n",
    "time_parallel = time.time() - start\n",
    "\n",
    "print(f\"\\nSerial: {time_serial:.3f}s\")\n",
    "print(f\"Parallel: {time_parallel:.3f}s\")\n",
    "print(f\"Speedup: {time_serial/time_parallel:.1f}x\")\n",
    "print(f\"Results match: {np.allclose(S_serial, S_parallel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel clustering with different random initializations\n",
    "def run_single_clustering(S, n_clusters, seed):\n",
    "    \"\"\"Run single clustering with specific seed.\"\"\"\n",
    "    ls = LocalSearch(n_clusters=n_clusters, n_restarts=1, random_state=seed)\n",
    "    clusters = ls.fit_predict(S)\n",
    "    return clusters, ls.objective_\n",
    "\n",
    "# Test data\n",
    "S_test = S_dense[:300, :300]\n",
    "n_runs = 20\n",
    "\n",
    "print(f\"Running {n_runs} clusterings in parallel...\")\n",
    "\n",
    "# Parallel execution\n",
    "start = time.time()\n",
    "parallel_results = Parallel(n_jobs=-1)(\n",
    "    delayed(run_single_clustering)(S_test, 5, seed) \n",
    "    for seed in range(n_runs)\n",
    ")\n",
    "time_parallel = time.time() - start\n",
    "\n",
    "# Find best result\n",
    "best_idx = np.argmin([obj for _, obj in parallel_results])\n",
    "best_clusters, best_objective = parallel_results[best_idx]\n",
    "\n",
    "print(f\"\\nCompleted in {time_parallel:.2f}s\")\n",
    "print(f\"Best objective: {best_objective:.3f}\")\n",
    "print(f\"Average time per run: {time_parallel/n_runs:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Approximate Methods <a id='approximate'></a>\n",
    "\n",
    "Trade accuracy for speed with approximate algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate nearest neighbors using LSH\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "class ApproximateSubstitutionMatrix:\n",
    "    \"\"\"Create approximate substitution matrix using random projections.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=50):\n",
    "        self.n_components = n_components\n",
    "        self.rp = GaussianRandomProjection(n_components=n_components)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        # Project to lower dimension\n",
    "        X_projected = self.rp.fit_transform(X)\n",
    "        \n",
    "        # Compute distances in lower dimension\n",
    "        S_approx = squareform(pdist(X_projected))\n",
    "        \n",
    "        return S_approx\n",
    "\n",
    "# Test approximation quality\n",
    "n_test = 1000\n",
    "X_test = np.random.randn(n_test, 50)  # Higher dimensional data\n",
    "\n",
    "print(\"Computing exact substitution matrix...\")\n",
    "start = time.time()\n",
    "S_exact = create_substitution_matrix(X_test)\n",
    "time_exact = time.time() - start\n",
    "\n",
    "print(\"\\nTesting different approximation levels:\")\n",
    "components = [10, 20, 30, 40]\n",
    "results = []\n",
    "\n",
    "for n_comp in components:\n",
    "    asm = ApproximateSubstitutionMatrix(n_components=n_comp)\n",
    "    \n",
    "    start = time.time()\n",
    "    S_approx = asm.fit_transform(X_test)\n",
    "    time_approx = time.time() - start\n",
    "    \n",
    "    # Calculate approximation error\n",
    "    error = np.mean(np.abs(S_exact - S_approx)) / np.mean(S_exact)\n",
    "    \n",
    "    results.append({\n",
    "        'components': n_comp,\n",
    "        'time': time_approx,\n",
    "        'speedup': time_exact / time_approx,\n",
    "        'error': error * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"  {n_comp} components: {time_approx:.3f}s (speedup: {time_exact/time_approx:.1f}x, error: {error*100:.1f}%)\")\n",
    "\n",
    "# Visualize trade-off\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(df_results['components'], df_results['speedup'], 'o-', markersize=8)\n",
    "ax1.set_xlabel('Number of Components')\n",
    "ax1.set_ylabel('Speedup Factor')\n",
    "ax1.set_title('Speedup vs Approximation Level')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(df_results['components'], df_results['error'], 's-', color='red', markersize=8)\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Approximation Error (%)')\n",
    "ax2.set_title('Error vs Approximation Level')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch Local Search for large datasets\n",
    "class FastMiniBatchLocalSearch:\n",
    "    \"\"\"Optimized mini-batch implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, batch_size=100, n_init=3, subsample_ratio=0.1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.batch_size = batch_size\n",
    "        self.n_init = n_init\n",
    "        self.subsample_ratio = subsample_ratio\n",
    "    \n",
    "    def fit_predict(self, S):\n",
    "        n = len(S)\n",
    "        \n",
    "        # Phase 1: Quick clustering on subsample\n",
    "        subsample_size = int(n * self.subsample_ratio)\n",
    "        subsample_idx = np.random.choice(n, subsample_size, replace=False)\n",
    "        S_sub = S[subsample_idx][:, subsample_idx]\n",
    "        \n",
    "        # Run standard algorithm on subsample\n",
    "        ls = LocalSearch(n_clusters=self.n_clusters, n_restarts=self.n_init)\n",
    "        clusters_sub = ls.fit_predict(S_sub)\n",
    "        \n",
    "        # Phase 2: Assign remaining points\n",
    "        clusters = np.zeros(n, dtype=int)\n",
    "        clusters[subsample_idx] = clusters_sub\n",
    "        \n",
    "        # Find cluster centers\n",
    "        centers = []\n",
    "        for k in range(self.n_clusters):\n",
    "            mask = subsample_idx[clusters_sub == k]\n",
    "            if len(mask) > 0:\n",
    "                center_idx = mask[np.argmin(S[mask][:, mask].sum(axis=1))]\n",
    "                centers.append(center_idx)\n",
    "            else:\n",
    "                centers.append(np.random.randint(n))\n",
    "        \n",
    "        # Assign remaining points to nearest center\n",
    "        unassigned = np.setdiff1d(np.arange(n), subsample_idx)\n",
    "        for i in unassigned:\n",
    "            distances = [S[i, c] for c in centers]\n",
    "            clusters[i] = np.argmin(distances)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "# Compare performance\n",
    "n_large = 2000\n",
    "X_large = np.random.randn(n_large, 30)\n",
    "S_large = create_substitution_matrix(X_large)\n",
    "\n",
    "print(f\"Clustering {n_large} products...\")\n",
    "\n",
    "# Standard approach\n",
    "start = time.time()\n",
    "ls_standard = LocalSearch(n_clusters=10, n_restarts=5)\n",
    "clusters_standard = ls_standard.fit_predict(S_large)\n",
    "time_standard = time.time() - start\n",
    "\n",
    "# Fast mini-batch\n",
    "start = time.time()\n",
    "mbls_fast = FastMiniBatchLocalSearch(n_clusters=10, subsample_ratio=0.2)\n",
    "clusters_fast = mbls_fast.fit_predict(S_large)\n",
    "time_fast = time.time() - start\n",
    "\n",
    "# Evaluate quality\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "agreement = adjusted_rand_score(clusters_standard, clusters_fast)\n",
    "\n",
    "print(f\"\\nStandard: {time_standard:.2f}s\")\n",
    "print(f\"Fast mini-batch: {time_fast:.2f}s\")\n",
    "print(f\"Speedup: {time_standard/time_fast:.1f}x\")\n",
    "print(f\"Agreement (ARI): {agreement:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking and Profiling <a id='benchmarking'></a>\n",
    "\n",
    "Tools for measuring and optimizing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmarking suite\n",
    "def benchmark_algorithm(algorithm, S, name, n_runs=5):\n",
    "    \"\"\"Benchmark an algorithm.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = algorithm.fit_predict(S)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times)\n",
    "    }\n",
    "\n",
    "# Test different algorithms\n",
    "n_bench = 500\n",
    "X_bench = np.random.randn(n_bench, 20)\n",
    "S_bench = create_substitution_matrix(X_bench)\n",
    "\n",
    "algorithms = [\n",
    "    (LocalSearch(n_clusters=5, n_restarts=10), \"Standard Local Search\"),\n",
    "    (LocalSearch(n_clusters=5, n_restarts=5), \"Local Search (5 restarts)\"),\n",
    "    (MiniBatchLocalSearch(n_clusters=5, batch_size=100), \"Mini-Batch Local Search\"),\n",
    "    (FastMiniBatchLocalSearch(n_clusters=5, subsample_ratio=0.3), \"Fast Mini-Batch\"),\n",
    "]\n",
    "\n",
    "print(\"Benchmarking algorithms...\")\n",
    "results = []\n",
    "\n",
    "for algo, name in algorithms:\n",
    "    result = benchmark_algorithm(algo, S_bench, name, n_runs=3)\n",
    "    results.append(result)\n",
    "    print(f\"{name}: {result['mean_time']:.3f}s Â± {result['std_time']:.3f}s\")\n",
    "\n",
    "# Visualize results\n",
    "df_bench = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(df_bench))\n",
    "plt.bar(x, df_bench['mean_time'], yerr=df_bench['std_time'], capsize=5)\n",
    "plt.xticks(x, df_bench['name'], rotation=45, ha='right')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling\n",
    "def memory_profile_function(func, *args, **kwargs):\n",
    "    \"\"\"Profile memory usage of a function.\"\"\"\n",
    "    mem_usage = memory_usage((func, args, kwargs))\n",
    "    return max(mem_usage) - min(mem_usage)\n",
    "\n",
    "# Test memory usage\n",
    "sizes = [100, 200, 500, 1000]\n",
    "memory_results = []\n",
    "\n",
    "print(\"Memory usage analysis:\")\n",
    "for n in sizes:\n",
    "    X = np.random.randn(n, 20).astype(np.float32)\n",
    "    \n",
    "    # Dense matrix memory\n",
    "    mem_dense = memory_profile_function(create_substitution_matrix, X)\n",
    "    \n",
    "    # Sparse matrix memory\n",
    "    mem_sparse = memory_profile_function(create_sparse_substitution_matrix, X, threshold=0.1)\n",
    "    \n",
    "    memory_results.append({\n",
    "        'n': n,\n",
    "        'dense_mb': mem_dense,\n",
    "        'sparse_mb': mem_sparse,\n",
    "        'ratio': mem_dense / mem_sparse if mem_sparse > 0 else np.inf\n",
    "    })\n",
    "    \n",
    "    print(f\"  n={n}: Dense={mem_dense:.1f}MB, Sparse={mem_sparse:.1f}MB, Ratio={memory_results[-1]['ratio']:.1f}x\")\n",
    "\n",
    "# Plot memory scaling\n",
    "df_mem = pd.DataFrame(memory_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_mem['n'], df_mem['dense_mb'], 'o-', label='Dense', markersize=8)\n",
    "plt.plot(df_mem['n'], df_mem['sparse_mb'], 's-', label='Sparse', markersize=8)\n",
    "plt.xlabel('Number of Products')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage Scaling')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance recommendations based on dataset size\n",
    "def recommend_approach(n_products, n_features, available_memory_gb):\n",
    "    \"\"\"Recommend optimization approach based on dataset characteristics.\"\"\"\n",
    "    \n",
    "    # Estimate memory requirements (MB)\n",
    "    dense_memory = (n_products ** 2 * 4) / 1e6  # float32\n",
    "    sparse_memory = dense_memory * 0.1  # Assume 90% sparsity\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Memory recommendations\n",
    "    if dense_memory < available_memory_gb * 1000 * 0.5:  # Use 50% of available\n",
    "        recommendations.append(\"âœ“ Dense matrix feasible\")\n",
    "    elif sparse_memory < available_memory_gb * 1000 * 0.5:\n",
    "        recommendations.append(\"âš  Use sparse matrix representation\")\n",
    "    else:\n",
    "        recommendations.append(\"âŒ Use memory-mapped arrays or distributed computing\")\n",
    "    \n",
    "    # Algorithm recommendations\n",
    "    if n_products < 1000:\n",
    "        recommendations.append(\"âœ“ Standard Local Search with multiple restarts\")\n",
    "    elif n_products < 10000:\n",
    "        recommendations.append(\"âš  Use Mini-Batch Local Search or parallel processing\")\n",
    "    else:\n",
    "        recommendations.append(\"âŒ Use approximate methods or subsampling\")\n",
    "    \n",
    "    # Optimization recommendations\n",
    "    if n_features > 100:\n",
    "        recommendations.append(\"ðŸ’¡ Consider dimensionality reduction\")\n",
    "    \n",
    "    if n_products > 5000:\n",
    "        recommendations.append(\"ðŸ’¡ Use parallel processing (-1 jobs)\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test recommendations\n",
    "test_cases = [\n",
    "    (500, 20, 8),\n",
    "    (5000, 50, 16),\n",
    "    (20000, 100, 32),\n",
    "    (100000, 200, 64)\n",
    "]\n",
    "\n",
    "print(\"Performance Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_prod, n_feat, mem_gb in test_cases:\n",
    "    print(f\"\\nDataset: {n_prod:,} products Ã— {n_feat} features, {mem_gb}GB RAM\")\n",
    "    recommendations = recommend_approach(n_prod, n_feat, mem_gb)\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Performance Optimization Strategies:\n",
    "\n",
    "1. **Memory Optimization**:\n",
    "   - Use sparse matrices when >80% zeros\n",
    "   - Memory-mapped arrays for very large datasets\n",
    "   - Float32 instead of float64 when precision allows\n",
    "\n",
    "2. **Computational Optimization**:\n",
    "   - Vectorize operations using NumPy\n",
    "   - JIT compilation with Numba for custom functions\n",
    "   - Avoid redundant computations\n",
    "\n",
    "3. **Parallel Processing**:\n",
    "   - Use all available cores with `n_jobs=-1`\n",
    "   - Parallel distance matrix computation\n",
    "   - Multiple random initializations in parallel\n",
    "\n",
    "4. **Approximate Methods**:\n",
    "   - Random projections for dimensionality reduction\n",
    "   - Subsampling for initial clustering\n",
    "   - Trade accuracy for speed when appropriate\n",
    "\n",
    "5. **Profiling and Monitoring**:\n",
    "   - Always profile before optimizing\n",
    "   - Monitor memory usage\n",
    "   - Benchmark different approaches\n",
    "\n",
    "### Decision Tree:\n",
    "\n",
    "- **< 1,000 products**: Use standard dense matrix\n",
    "- **1,000 - 10,000 products**: Consider sparse matrices and parallel processing\n",
    "- **10,000 - 100,000 products**: Use mini-batch algorithms and approximations\n",
    "- **> 100,000 products**: Distributed computing or specialized big data tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}