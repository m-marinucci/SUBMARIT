{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced SUBMARIT Usage\n",
    "\n",
    "This notebook demonstrates advanced features and techniques for submarket analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Working with Real Data](#real-data)\n",
    "2. [Custom Distance Metrics](#custom-metrics)\n",
    "3. [Cross-Validation and Stability](#validation)\n",
    "4. [Performance Optimization](#performance)\n",
    "5. [MATLAB Integration](#matlab)\n",
    "6. [Advanced Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "# SUBMARIT imports\n",
    "from submarit.core import create_substitution_matrix, create_sparse_substitution_matrix\n",
    "from submarit.algorithms import LocalSearch, MiniBatchLocalSearch\n",
    "from submarit.evaluation import ClusterEvaluator, gap_statistic, stability_analysis\n",
    "from submarit.validation import KFoldValidator, MultipleRunsValidator\n",
    "from submarit.io import load_data, save_results, load_matlab_data\n",
    "from submarit.utils.matlab_compat import MatlabAPI\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Advanced SUBMARIT tutorial ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with Real Data <a id='real-data'></a>\n",
    "\n",
    "Let's work with a realistic retail dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic retail product dataset\n",
    "n_products = 500\n",
    "\n",
    "# Product categories\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Home', 'Sports']\n",
    "brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "\n",
    "# Generate product data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'product_id': [f'SKU_{i:04d}' for i in range(n_products)],\n",
    "    'category': np.random.choice(categories, n_products, p=[0.3, 0.2, 0.2, 0.2, 0.1]),\n",
    "    'brand': np.random.choice(brands, n_products),\n",
    "    'price': np.random.lognormal(3.5, 0.8, n_products),\n",
    "    'rating': np.random.normal(4.0, 0.5, n_products).clip(1, 5),\n",
    "    'reviews': np.random.negative_binomial(5, 0.1, n_products),\n",
    "    'in_stock': np.random.choice([0, 1], n_products, p=[0.1, 0.9]),\n",
    "    'discount_pct': np.random.choice([0, 5, 10, 15, 20], n_products, p=[0.5, 0.2, 0.15, 0.1, 0.05])\n",
    "}\n",
    "\n",
    "# Add category-specific features\n",
    "for i in range(n_products):\n",
    "    if data['category'][i] == 'Electronics':\n",
    "        data['price'][i] *= 5  # Electronics are more expensive\n",
    "    elif data['category'][i] == 'Food':\n",
    "        data['price'][i] *= 0.3  # Food is cheaper\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "# One-hot encode categorical variables\n",
    "category_dummies = pd.get_dummies(df['category'], prefix='cat')\n",
    "brand_dummies = pd.get_dummies(df['brand'], prefix='brand')\n",
    "\n",
    "# Normalize numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_features = ['price', 'rating', 'reviews', 'discount_pct']\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[numerical_features])\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=[f'{col}_scaled' for col in numerical_features])\n",
    "\n",
    "# Combine all features\n",
    "X = pd.concat([scaled_df, category_dummies, brand_dummies], axis=1).values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {list(scaled_df.columns) + list(category_dummies.columns) + list(brand_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Distance Metrics <a id='custom-metrics'></a>\n",
    "\n",
    "Create custom distance metrics for specific business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom distance function\n",
    "def weighted_product_distance(x, y, feature_weights):\n",
    "    \"\"\"\n",
    "    Custom distance that weights features differently.\n",
    "    Price and category are more important for substitution.\n",
    "    \"\"\"\n",
    "    diff = x - y\n",
    "    weighted_diff = diff * feature_weights\n",
    "    return np.sqrt(np.sum(weighted_diff ** 2))\n",
    "\n",
    "# Create feature weights (price and category more important)\n",
    "n_features = X.shape[1]\n",
    "feature_weights = np.ones(n_features)\n",
    "feature_weights[0] = 3.0  # Price weight\n",
    "feature_weights[4:9] = 2.0  # Category weights\n",
    "\n",
    "# Create custom substitution matrix\n",
    "def create_custom_substitution_matrix(X, feature_weights):\n",
    "    n = len(X)\n",
    "    S = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = weighted_product_distance(X[i], X[j], feature_weights)\n",
    "            S[i, j] = S[j, i] = dist\n",
    "    \n",
    "    return S\n",
    "\n",
    "# For efficiency, use vectorized version\n",
    "def create_custom_substitution_matrix_fast(X, feature_weights):\n",
    "    # Weight the features\n",
    "    X_weighted = X * np.sqrt(feature_weights)\n",
    "    # Use scipy's pdist for efficiency\n",
    "    distances = pdist(X_weighted, metric='euclidean')\n",
    "    return squareform(distances)\n",
    "\n",
    "# Compare standard vs custom\n",
    "S_standard = create_substitution_matrix(X, metric='euclidean')\n",
    "S_custom = create_custom_substitution_matrix_fast(X, feature_weights)\n",
    "\n",
    "print(f\"Standard matrix - Mean distance: {S_standard.mean():.3f}\")\n",
    "print(f\"Custom matrix - Mean distance: {S_custom.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation and Stability <a id='validation'></a>\n",
    "\n",
    "Ensure clustering results are stable and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation\n",
    "validator = KFoldValidator(n_splits=5, random_state=42)\n",
    "\n",
    "# Test different numbers of clusters\n",
    "k_values = [3, 5, 7, 10]\n",
    "cv_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nValidating k={k}...\")\n",
    "    scores = validator.validate(X, n_clusters=k, algorithm='local_search')\n",
    "    cv_results[k] = {\n",
    "        'scores': scores,\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "    print(f\"CV Score: {cv_results[k]['mean']:.3f} Â± {cv_results[k]['std']:.3f}\")\n",
    "\n",
    "# Visualize CV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "means = [cv_results[k]['mean'] for k in k_values]\n",
    "stds = [cv_results[k]['std'] for k in k_values]\n",
    "\n",
    "plt.errorbar(k_values, means, yerr=stds, marker='o', markersize=10, capsize=5)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Cross-Validation Score')\n",
    "plt.title('Cross-Validation Results')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability analysis - multiple runs\n",
    "n_clusters = 5\n",
    "n_runs = 20\n",
    "\n",
    "# Run clustering multiple times\n",
    "all_clusterings = []\n",
    "for run in range(n_runs):\n",
    "    ls = LocalSearch(n_clusters=n_clusters, n_restarts=5, random_state=run)\n",
    "    clusters = ls.fit_predict(S_custom)\n",
    "    all_clusterings.append(clusters)\n",
    "\n",
    "# Calculate pairwise agreement\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "agreement_matrix = np.zeros((n_runs, n_runs))\n",
    "for i in range(n_runs):\n",
    "    for j in range(n_runs):\n",
    "        agreement_matrix[i, j] = adjusted_rand_score(all_clusterings[i], all_clusterings[j])\n",
    "\n",
    "# Visualize stability\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(agreement_matrix, cmap='Blues', vmin=0, vmax=1, \n",
    "            annot=False, square=True, cbar_kws={'label': 'Adjusted Rand Index'})\n",
    "plt.title('Clustering Stability Matrix')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Run')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average stability (ARI): {agreement_matrix[np.triu_indices(n_runs, k=1)].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Optimization <a id='performance'></a>\n",
    "\n",
    "Techniques for handling large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for performance testing\n",
    "n_large = 2000\n",
    "X_large = np.random.randn(n_large, 50)\n",
    "\n",
    "# Compare different approaches\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Dense matrix\n",
    "start = time.time()\n",
    "S_dense = create_substitution_matrix(X_large, metric='euclidean')\n",
    "time_dense = time.time() - start\n",
    "print(f\"Dense matrix creation: {time_dense:.2f}s\")\n",
    "print(f\"Memory usage: {S_dense.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "# 2. Sparse matrix (keep only top 10%)\n",
    "start = time.time()\n",
    "S_sparse = create_sparse_substitution_matrix(X_large, threshold=0.1, metric='euclidean')\n",
    "time_sparse = time.time() - start\n",
    "print(f\"\\nSparse matrix creation: {time_sparse:.2f}s\")\n",
    "print(f\"Memory usage: {(S_sparse.data.nbytes + S_sparse.indices.nbytes + S_sparse.indptr.nbytes) / 1e6:.1f} MB\")\n",
    "print(f\"Sparsity: {1 - S_sparse.nnz / (n_large**2):.1%}\")\n",
    "\n",
    "# 3. Mini-batch clustering\n",
    "print(\"\\nClustering comparison:\")\n",
    "\n",
    "# Standard Local Search\n",
    "start = time.time()\n",
    "ls_standard = LocalSearch(n_clusters=10, n_restarts=3)\n",
    "clusters_standard = ls_standard.fit_predict(S_dense[:1000, :1000])  # Subset for speed\n",
    "time_standard = time.time() - start\n",
    "print(f\"Standard Local Search: {time_standard:.2f}s\")\n",
    "\n",
    "# Mini-batch Local Search\n",
    "start = time.time()\n",
    "mbls = MiniBatchLocalSearch(n_clusters=10, batch_size=100, n_init=3)\n",
    "clusters_minibatch = mbls.fit_predict(S_sparse[:1000, :1000])\n",
    "time_minibatch = time.time() - start\n",
    "print(f\"Mini-batch Local Search: {time_minibatch:.2f}s\")\n",
    "print(f\"Speedup: {time_standard/time_minibatch:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing example\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Available cores: {n_cores}\")\n",
    "\n",
    "# Function to run single clustering\n",
    "def run_clustering(S, k, seed):\n",
    "    ls = LocalSearch(n_clusters=k, n_restarts=1, random_state=seed)\n",
    "    return ls.fit_predict(S), ls.objective_\n",
    "\n",
    "# Serial execution\n",
    "start = time.time()\n",
    "serial_results = []\n",
    "for seed in range(10):\n",
    "    result = run_clustering(S_dense[:500, :500], 5, seed)\n",
    "    serial_results.append(result)\n",
    "time_serial = time.time() - start\n",
    "\n",
    "# Parallel execution\n",
    "start = time.time()\n",
    "parallel_results = Parallel(n_jobs=-1)(\n",
    "    delayed(run_clustering)(S_dense[:500, :500], 5, seed) \n",
    "    for seed in range(10)\n",
    ")\n",
    "time_parallel = time.time() - start\n",
    "\n",
    "print(f\"\\nSerial execution: {time_serial:.2f}s\")\n",
    "print(f\"Parallel execution: {time_parallel:.2f}s\")\n",
    "print(f\"Speedup: {time_serial/time_parallel:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MATLAB Integration <a id='matlab'></a>\n",
    "\n",
    "Working with MATLAB files and ensuring compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data to save in MATLAB format\n",
    "matlab_data = {\n",
    "    'features': X[:100],  # First 100 products\n",
    "    'substitution_matrix': S_custom[:100, :100],\n",
    "    'product_names': np.array(df['product_id'][:100].values, dtype=object),\n",
    "    'parameters': {\n",
    "        'n_clusters': 5,\n",
    "        'metric': 'custom',\n",
    "        'algorithm': 'local_search'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to MATLAB format\n",
    "from scipy.io import savemat\n",
    "savemat('sample_data.mat', matlab_data)\n",
    "print(\"Data saved to sample_data.mat\")\n",
    "\n",
    "# Load and verify\n",
    "from scipy.io import loadmat\n",
    "loaded_data = loadmat('sample_data.mat')\n",
    "print(\"\\nLoaded data keys:\", [k for k in loaded_data.keys() if not k.startswith('__')])\n",
    "print(f\"Features shape: {loaded_data['features'].shape}\")\n",
    "print(f\"First product name: {loaded_data['product_names'][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MATLAB compatibility API\n",
    "matlab_api = MatlabAPI()\n",
    "\n",
    "# MATLAB-style function calls\n",
    "S_matlab_style = matlab_api.create_substitution_matrix(X[:100], 'euclidean')\n",
    "clusters_matlab_style = matlab_api.local_search(S_matlab_style, 5)\n",
    "\n",
    "# Convert to 1-based indexing for MATLAB\n",
    "clusters_matlab_indexed = clusters_matlab_style + 1\n",
    "\n",
    "print(f\"Cluster range (Python): {clusters_matlab_style.min()}-{clusters_matlab_style.max()}\")\n",
    "print(f\"Cluster range (MATLAB): {clusters_matlab_indexed.min()}-{clusters_matlab_indexed.max()}\")\n",
    "\n",
    "# Save results in MATLAB format\n",
    "matlab_results = {\n",
    "    'clusters': clusters_matlab_indexed,\n",
    "    'substitution_matrix': S_matlab_style,\n",
    "    'objective_value': np.array([42.0]),  # Example objective value\n",
    "    'n_iterations': np.array([15])\n",
    "}\n",
    "savemat('results_for_matlab.mat', matlab_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Visualization <a id='visualization'></a>\n",
    "\n",
    "Create publication-quality visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering for visualization\n",
    "ls = LocalSearch(n_clusters=5, n_restarts=10, random_state=42)\n",
    "final_clusters = ls.fit_predict(S_custom)\n",
    "df['cluster'] = final_clusters\n",
    "\n",
    "# 1. Interactive cluster exploration\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare data for 3D visualization\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_3d = pca.fit_transform(X)\n",
    "\n",
    "# Create interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    x=X_3d[:, 0], y=X_3d[:, 1], z=X_3d[:, 2],\n",
    "    color=final_clusters,\n",
    "    hover_data={'Product': df['product_id'], \n",
    "                'Category': df['category'],\n",
    "                'Price': df['price'].round(2)},\n",
    "    labels={'x': f'PC1 ({pca.explained_variance_ratio_[0]:.1%})',\n",
    "            'y': f'PC2 ({pca.explained_variance_ratio_[1]:.1%})',\n",
    "            'z': f'PC3 ({pca.explained_variance_ratio_[2]:.1%})',\n",
    "            'color': 'Cluster'},\n",
    "    title='Interactive 3D Cluster Visualization'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster profile radar chart\n",
    "# Calculate average features per cluster\n",
    "feature_cols = ['price', 'rating', 'reviews', 'discount_pct']\n",
    "cluster_profiles = df.groupby('cluster')[feature_cols].mean()\n",
    "\n",
    "# Normalize to 0-1 scale for radar chart\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cluster_profiles_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(cluster_profiles),\n",
    "    index=cluster_profiles.index,\n",
    "    columns=cluster_profiles.columns\n",
    ")\n",
    "\n",
    "# Create radar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster in range(5):\n",
    "    values = cluster_profiles_norm.loc[cluster].values.tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=feature_cols + [feature_cols[0]],\n",
    "        fill='toself',\n",
    "        name=f'Cluster {cluster}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(visible=True, range=[0, 1])\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    title=\"Cluster Profiles (Normalized Features)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hierarchical cluster visualization\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Compute linkage matrix\n",
    "linkage_matrix = linkage(S_custom[:100, :100], method='ward')\n",
    "\n",
    "# Create dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=df['product_id'][:100].values,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=8\n",
    ")\n",
    "plt.title('Product Hierarchy (First 100 Products)', fontsize=16)\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Network visualization of product relationships\n",
    "import networkx as nx\n",
    "\n",
    "# Create network from substitution matrix\n",
    "# Keep only strong connections (top 5% most similar)\n",
    "threshold = np.percentile(S_custom[S_custom > 0], 5)\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for i in range(100):  # Use first 100 products\n",
    "    G.add_node(i, \n",
    "               label=df.iloc[i]['product_id'],\n",
    "               cluster=final_clusters[i],\n",
    "               category=df.iloc[i]['category'])\n",
    "\n",
    "# Add edges for similar products\n",
    "for i in range(100):\n",
    "    for j in range(i+1, 100):\n",
    "        if S_custom[i, j] < threshold:\n",
    "            G.add_edge(i, j, weight=1/S_custom[i, j])\n",
    "\n",
    "# Plot network\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "# Draw nodes colored by cluster\n",
    "node_colors = [final_clusters[i] for i in range(100)]\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                      cmap='viridis', node_size=300, alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "\n",
    "# Add labels for some nodes\n",
    "labels = {i: G.nodes[i]['label'][:8] for i in range(0, 100, 10)}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "\n",
    "plt.title('Product Similarity Network', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Network has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Publication-quality heatmap\n",
    "# Prepare data for heatmap\n",
    "cluster_category_counts = pd.crosstab(df['cluster'], df['category'])\n",
    "cluster_brand_counts = pd.crosstab(df['cluster'], df['brand'])\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Category distribution\n",
    "sns.heatmap(cluster_category_counts, annot=True, fmt='d', \n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Product Count'},\n",
    "            ax=ax1)\n",
    "ax1.set_title('Products per Category in Each Cluster', fontsize=14)\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Cluster')\n",
    "\n",
    "# Brand distribution\n",
    "sns.heatmap(cluster_brand_counts, annot=True, fmt='d',\n",
    "            cmap='YlGnBu', cbar_kws={'label': 'Product Count'},\n",
    "            ax=ax2)\n",
    "ax2.set_title('Products per Brand in Each Cluster', fontsize=14)\n",
    "ax2.set_xlabel('Brand')\n",
    "ax2.set_ylabel('Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Data Preparation**: \n",
    "   - Properly encode categorical variables\n",
    "   - Scale numerical features\n",
    "   - Consider feature importance when creating distance metrics\n",
    "\n",
    "2. **Custom Metrics**:\n",
    "   - Create domain-specific distance functions\n",
    "   - Weight features based on business importance\n",
    "\n",
    "3. **Validation**:\n",
    "   - Always use cross-validation\n",
    "   - Check clustering stability\n",
    "   - Test multiple k values\n",
    "\n",
    "4. **Performance**:\n",
    "   - Use sparse matrices for large datasets\n",
    "   - Leverage parallel processing\n",
    "   - Consider mini-batch algorithms\n",
    "\n",
    "5. **Visualization**:\n",
    "   - Use multiple perspectives\n",
    "   - Create interactive visualizations for exploration\n",
    "   - Generate publication-quality figures\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these techniques to your own data\n",
    "- Experiment with different distance metrics\n",
    "- Combine with business rules for practical applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('sample_data.mat'):\n",
    "    os.remove('sample_data.mat')\n",
    "if os.path.exists('results_for_matlab.mat'):\n",
    "    os.remove('results_for_matlab.mat')\n",
    "    \n",
    "print(\"Advanced tutorial completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}